"""
Copyright 2020 Michael Schmid, Genexa AG (michael.schmid@genexa.ch)
https://github.com/ms-gx/circaidme

This file is part of CircAidMe. CircAidMe is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by the Free Software Foundation,
either version 3 of the License, or (at your option) any later version. CircAidMe is distributed in
the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details. You should have received a copy of the GNU General Public License along with CircAidMe. If
not, see <http://www.gnu.org/licenses/>.
"""

# own modules
from circaidme import log
from circaidme import parameter
from circaidme import cpp_function_wrappers

# external modules
import argparse
import os
import shutil
import math
import subprocess
import multiprocessing
import pandas as pd
import operator
from Bio.Seq import Seq
from Bio.Seq import IUPAC
from Bio import AlignIO


# class "Stat" is used to collect and handle statistics about the analysis:
class Stat:
	@staticmethod
	def inc_key(key, stats):
		stats[key] += 1

	@staticmethod
	def write_stats(stats, stats_overall_per_read, log_object):
		log_object.add_line("*** Main statistics *** ")
		log_object.add_line("Input ONT reads: " + str(stats["cnt_in_reads"]))
		log_object.add_line("Input ONT reads that were too short: " + str(stats["cnt_short_in_reads"]))
		log_object.add_line("Input ONT reads that got split up: " + str(stats["cnt_split_in_reads"]))
		log_object.add_line("Input ONT reads that did not get split up: " + str(stats["cnt_non_split_in_reads"]))
		log_object.add_line("Subreads generated by splitting input reads: " + str(stats["cnt_split_generated_reads"]))
		log_object.add_line("Number of proper consensus sequences that were generated: " + str(stats["cnt_proper_consensus"]) + "\n")
		log_object.add_line("*** Statistics about (sub-)reads which got discarded *** ")
		log_object.add_line("Reads with fewer than two adapters (no insert can be identified): " + str(stats["fewer_two_adapters_found"]))
		log_object.add_line("Reads with fewer inserts than parameter \"min-inserts\": " + str(stats["fewer_min_inserts_found"]))
		log_object.add_line("Reads with only forward inserts: " + str(stats["only_forward_inserts"]))
		log_object.add_line("Reads with problematic insert orientation: " + str(stats["problematic_insert_orientation"]))
		log_object.add_line("Reads resulting in low quality MSA: " + str(stats["bad_MSA"]))
		log_object.add_line("Reads where the adapter self-circularized or is contained in consensus: " + str(stats["adapter_as_insert"]))
		log_object.add_line("Reads with subreads too short after split: " + str(stats["no_minlen_subread_after_split"]))
		log_object.add_line("Consensus length out of range: " + str(stats["consensus_size_out_of_range"]) + "\n")
		log_object.add_line("*** Length- and count-statistics *** ")
		log_object.add_line("                      |    Mean |  Median |   Min |   Max |")
		log_object.add_line("Input read length     |%8.2f |%8.2f |%6d |%6d |" % (stats_overall_per_read[0][0], stats_overall_per_read[0][1], stats_overall_per_read[0][2], stats_overall_per_read[0][3]))
		log_object.add_line("Number of adapters    |%8.2f |%8.2f |%6d |%6d |" % (stats_overall_per_read[1][0], stats_overall_per_read[1][1], stats_overall_per_read[1][2], stats_overall_per_read[1][3]))
		log_object.add_line("Number of inserts     |%8.2f |%8.2f |%6d |%6d |" % (stats_overall_per_read[2][0], stats_overall_per_read[2][1], stats_overall_per_read[2][2], stats_overall_per_read[2][3]))
		log_object.add_line("Cons. len. (all)      |%8.2f |%8.2f |%6d |%6d |" % (stats_overall_per_read[3][0], stats_overall_per_read[3][1], stats_overall_per_read[3][2], stats_overall_per_read[3][3]))
		log_object.add_line("Cons. len. (proper)   |%8.2f |%8.2f |%6d |%6d |\n" % (stats_overall_per_read[4][0], stats_overall_per_read[4][1], stats_overall_per_read[4][2], stats_overall_per_read[4][3]))
		log_object.add_line("*** Statistics directionality *** ")
		log_object.add_line("(Forward reads might get discarded if set accordingly. See above if discarded.)")
		log_object.add_line("Forward: " + str(stats_overall_per_read[5][0]) + ", Reverse: " + str(stats_overall_per_read[5][1])  + ", Both_FR: " + str(stats_overall_per_read[5][2])  + ", Both_RF: " + str(stats_overall_per_read[5][3]) + ", Problematic: " + str(stats_overall_per_read[5][4]) + "\n")

	@staticmethod
	def init_read_stat(stats_per_read, name):
		stats_per_read[name] = {
			"len_read": None,
			"len_cons": None,
			"split": None, # False -> Non-split, True -> split
			"split_reason": None,
			"adapter": None,
			"dir": None,
			"nr_adapters": None,
			"nr_inserts": None,
			"final_state": None
		}

	@staticmethod
	def add_data_read_stat(stats_per_read, name, key, value):
		tmp = stats_per_read[name]
		tmp[key] = value
		stats_per_read[name] = tmp

	@staticmethod
	def print_read_stats(stats_per_read, out_csv):
		df = pd.DataFrame(columns=['name_read', 'len_read', 'split', 'split_reason', 'adapter', 'nr_adapters', 'nr_inserts', 'dir', 'len_cons', 'final_state'], index=range(len(stats_per_read)))

		for counter, key in enumerate(stats_per_read):
			df.at[counter, 'name_read'] = key
			df.at[counter, 'len_read'] = stats_per_read[key]["len_read"]
			df.at[counter, 'split'] = stats_per_read[key]["split"]
			df.at[counter, 'split_reason'] = stats_per_read[key]["split_reason"]
			df.at[counter, 'adapter'] = stats_per_read[key]["adapter"]
			df.at[counter, 'nr_adapters'] = stats_per_read[key]["nr_adapters"]
			df.at[counter, 'nr_inserts'] = stats_per_read[key]["nr_inserts"]
			df.at[counter, 'dir'] = stats_per_read[key]["dir"]
			df.at[counter, 'len_cons'] = stats_per_read[key]["len_cons"]
			df.at[counter, 'final_state'] = stats_per_read[key]["final_state"]

		df.to_csv(out_csv, sep=',', )

		len_read_mean = df['len_read'].mean()
		len_read_median = df['len_read'].median()
		len_read_min = df['len_read'].min()
		len_read_max = df['len_read'].max()
		nr_adapters_mean = df['nr_adapters'].mean()
		nr_adapters_median = df['nr_adapters'].median()
		nr_adapters_min = df['nr_adapters'].min()
		nr_adapters_max = df['nr_adapters'].max()
		nr_inserts_mean = df['nr_inserts'].mean()
		nr_inserts_median = df['nr_inserts'].median()
		nr_inserts_min = df['nr_inserts'].min()
		nr_inserts_max = df['nr_inserts'].max()
		len_cons_mean_propper = df[df.final_state == "consensus"]['len_cons'].mean()
		len_cons_median_propper = df[df.final_state == "consensus"]['len_cons'].median()
		len_cons_min_propper = df[df.final_state == "consensus"]['len_cons'].min()
		len_cons_max_propper = df[df.final_state == "consensus"]['len_cons'].max()
		len_cons_mean = df['len_cons'].mean()
		len_cons_median = df['len_cons'].median()
		len_cons_min = df['len_cons'].min()
		len_cons_max = df['len_cons'].max()
		dir_count_fwd = df[df.dir == "I_FORWARD"].shape[0]
		dir_count_rev = df[df.dir == "I_REVERSE"].shape[0]
		dir_count_both_fr = df[df.dir == "I_BOTH_FR"].shape[0]
		dir_count_rf = df[df.dir == "I_BOTH_RF"].shape[0]
		dir_count_rf = df[df.dir == "I_BOTH_RF"].shape[0]
		dir_count_problematic = df[df.dir == "I_PROBLEMATIC"].shape[0]

		return_values = [
			[len_read_mean, len_read_median, len_read_min, len_read_max],
			[nr_adapters_mean, nr_adapters_median, nr_adapters_min, nr_adapters_max],
			[nr_inserts_mean, nr_inserts_median, nr_inserts_min, nr_inserts_max],
			[len_cons_mean, len_cons_median, len_cons_min, len_cons_max],
			[len_cons_mean_propper, len_cons_median_propper, len_cons_min_propper, len_cons_max_propper],
			[dir_count_fwd, dir_count_rev, dir_count_both_fr, dir_count_rf, dir_count_problematic]
		]

		# replace NaN by zero (otherwise the logfile can not be written):		
		for num_out,outer in enumerate(return_values):
			for num_in,inner in enumerate(outer):
				if(inner != inner):
					return_values[num_out][num_in] = 0

		return(return_values)

			
# class "Match" is used to store single alignments of the adapter to an ONT read:
class Match:
	def __init__(self, score = 0, orientation = 0, start_query = 0, end_query = 0, start_adapter = 0, end_adapter = 0, left_adjust = 0, right_adjust = 0, adapter_name = "non_provided"):
		self.score = int(score) # not used
		self.orientation = orientation
		self.start_query = int(start_query) + left_adjust
		self.end_query = int(end_query) + right_adjust
		self.start_adapter = int(start_adapter) # not used
		self.end_adapter = int(end_adapter) # not used
		self.maplen_adapter = self.end_adapter - self.start_adapter + 1 # not used
		self.adapter_name = adapter_name


# class "Insert" is used to store single inserts between two adapters:
class Insert:
	def __init__(self, orientation = 0, start_query = 0, end_query = 0):
		self.orientation = orientation
		self.start_query = int(start_query)
		self.end_query = int(end_query)


# class "Aligments" stores all the alignment data and contains all the functions to generate the consensus:
class Alignments:
	def takeQueryStart(self, one_match):
		return one_match.start_query

	def __init__(self, read_id, read_desc, split):
		self.read_id = read_id
		self.read_desc = read_desc
		self.split = split # stores if this read was split because we had evidence for read fusion
		self.track_adapters = ""
		self.track_inserts = ""
		self.matches = [] # holds the matching postitions of adapters to the ONT read
		self.inserts = [] # holds all the extracted inserts

	def add_match(self, match, left_adjust, right_adjust, adapter_name = "non_provided"): # function adds a adapter match to the list "matches"
		self.matches.append(Match(match[0], match[1], match[2], match[3], match[4], match[5], left_adjust, right_adjust, adapter_name))

	def sort(self): # function sorts the matches according to their starting pos on the ONT read
		self.matches.sort(key=self.takeQueryStart)

	def fill_inserts(self): # extract inserts and fill them to list "inserts"
		last_end = 0 # storing of this two values could be performed more generic via storing a object
		last_orient_adapter = ""
		last_orient_insert = ""
		orient_changes = 0
		track_adapters = ""
		track_inserts = ""
		for num,match in enumerate(self.matches):
			if(num == 0):
				last_end = match.end_query
				last_orient_adapter = match.orientation
				track_adapters += match.orientation
			else:
				if(last_orient_adapter != match.orientation):
					last_orient_adapter = match.orientation
					last_end = match.end_query
					orient_changes += 1
					track_adapters += match.orientation	
					continue
				if( ( (match.start_query-last_end) < parameter.insert_min_len) or
			            ( (match.start_query-last_end) > parameter.insert_max_len) ):
					continue
				self.inserts.append(Insert(match.orientation, last_end, match.start_query))
				if(last_orient_insert != match.orientation):
					track_inserts += match.orientation
					last_orient_insert = match.orientation
				last_end = match.end_query

		if(track_adapters == '-'):
			self.track_adapters = "L_REVERSE"
		elif(track_adapters == '+'):
			self.track_adapters = "L_FORWARD"
		elif(track_adapters == '-+'):
			self.track_adapters = "L_BOTH_RF"
		elif(track_adapters == '+-'):
			self.track_adapters = "L_BOTH_FR"
		else:
			self.track_adapters = "L_PROBLEMATIC"

		if(track_inserts == '-'):
			self.track_inserts = "I_REVERSE"
		elif(track_inserts == '+'):
			self.track_inserts = "I_FORWARD"
		elif(track_inserts == '-+'):
			self.track_inserts = "I_BOTH_RF"
		elif(track_inserts == '+-'):
			self.track_inserts = "I_BOTH_FR"
		else:
			self.track_inserts = "I_PROBLEMATIC"

	def check_fused_read(self): # check for fused reads
		last_orient = ""
		last_adapter = ""
		orient_changes = 0
		cut_positions = []
		cut_types = set()
		for num,match in enumerate(self.matches):
			if(num == 0):
				last_orient = match.orientation
				last_adapter = match.adapter_name
			else:			
				if(last_orient != match.orientation):
					last_orient = match.orientation
					orient_changes += 1		
				if(last_adapter != match.adapter_name):
					cut_types.add("adapter_type")
					cut_positions.append(max(match.start_query-1,0))
					orient_changes = 0
					last_adapter = match.adapter_name
				if(orient_changes > 1):
					cut_types.add("adapter_orientation")
					cut_positions.append(max(match.start_query-1,0))
					orient_changes = 0
					last_adapter = match.adapter_name
				last_adapter = match.adapter_name
		if(len(cut_positions) == 0): return(([-1],["none"]))
		else: return ((cut_positions,list(cut_types)))


	def muscle(self, outpath, filtered, maxiters): # function used to execute MUSCLE
		if(filtered == True):
			add_ext = "_filt"
		else:
			add_ext = ""
		command = os.path.dirname(os.path.abspath(__file__)) + "/" + parameter.path_muscle + " -diags -maxiters " + str(maxiters) + " -quiet -clw" + " -in " + outpath + "/tmp_work_dir/" + self.read_id + "_inserts.fasta" + add_ext + " -out " + outpath + "/tmp_work_dir/" + self.read_id + ".clw" + add_ext
		process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)
		output, error = process.communicate()


	def cons(self, outpath): # own function to call consensus from MSA. Using biopython to parse Clustal file.
		align = AlignIO.read(outpath + "/tmp_work_dir/" + self.read_id + ".clw_filt", "clustal")
		count_align = len(align)

		if(count_align > 12):
			identity = 6
		elif(count_align > 9):
			identity = 5
		elif(count_align > 6):
			identity = 4			
		elif(count_align > 4):
			identity = 3
		else:
			identity = 2

		no_gap_threshold = math.ceil(count_align*0.34)
		gap_threshold = math.ceil(count_align*0.45)

		cons = ""
		dict_cons = {}
		for col in range(0,align.get_alignment_length()):
			cln = align[:, col]			
			dict_cons["A"] = cln.count('A') + cln.count('a')
			dict_cons["C"] = cln.count('C') + cln.count('c')
			dict_cons["G"] = cln.count('G') + cln.count('g')
			dict_cons["T"] = cln.count('T') + cln.count('t')
			dict_cons["-"] = cln.count('-')
			if((dict_cons["-"] > 0) and (max(dict_cons.items(), key=operator.itemgetter(1))[1] >= gap_threshold)):
				cons = cons + max(dict_cons.items(), key=operator.itemgetter(1))[0]
			elif((dict_cons["-"] == 0) and (max(dict_cons.items(), key=operator.itemgetter(1))[1] >= no_gap_threshold)):
				cons = cons + max(dict_cons.items(), key=operator.itemgetter(1))[0]
		
		with open(outpath + "/tmp_work_dir/" + self.read_id + ".cons", 'a') as out_f:
			out_f.write(">cons\n" )
			out_f.write(cons + "\n")


	def filter_good_align(self, outpath): # filters the inserts which have a "nice alignment" in the MSA. Using "esl-alipid" form here: https://github.com/EddyRivasLab/easel/blob/master/miniapps/esl-alipid.c
		command = os.path.dirname(os.path.abspath(__file__)) + "/" + parameter.path_esl_alipid + " --noheader --dna " + outpath + "/tmp_work_dir/" + self.read_id + ".clw | tr -s \" \" > " + outpath + "/tmp_work_dir/" + self.read_id + ".ids"
		process = subprocess.Popen(command, shell=True)
		output, error = process.communicate()

		try:		
			percent_col = pd.read_csv(outpath + "/tmp_work_dir/" + self.read_id + ".ids", header=None, usecols=[2], sep=" ")
		except:
			return(-1)

		sum_of_ident = percent_col[percent_col[2] > 96.0].count()[2]

		if(sum_of_ident > 6):
			pid = "85.0"; pmatch = "85.0"
		elif(sum_of_ident > 2):
			pid = "80.0"; pmatch = "80.0"
		else:
			pid = "75.0"; pmatch = "75.0"

		command = "cat " + outpath + "/tmp_work_dir/" + self.read_id + ".ids | awk '$3 > " + pid + " && $6 > " + pmatch + "' | tr -s \" \" | cut -d \" \" -f1-2 | awk '{print $1 \"\\n\" $2}' | sort | uniq > " + outpath + "/tmp_work_dir/" + self.read_id + ".filt_ids"
		process = subprocess.Popen(command, shell=True)
		output, error = process.communicate()

		if(os.stat(outpath + "/tmp_work_dir/" + self.read_id + ".filt_ids").st_size == 0): # we try to rescue the ones that slipped through above
			command = "cat " + outpath + "/tmp_work_dir/" + self.read_id + ".ids | awk '$3 > 70.0 && $6 > 70.0' | tr -s \" \" | cut -d \" \" -f1-2 | awk '{print $1 \"\\n\" $2}' | sort | uniq > " + outpath + "/tmp_work_dir/" + self.read_id + ".filt_ids"
			process = subprocess.Popen(command, shell=True)
			output, error = process.communicate()
	
		if(os.stat(outpath + "/tmp_work_dir/" + self.read_id + ".filt_ids").st_size == 0):
			return(-1) # we skip this read in this case
		else:	
			command = "cat " + outpath + "/tmp_work_dir/" + self.read_id + "_inserts.fasta | " + os.path.dirname(os.path.abspath(__file__)) + "/" + parameter.path_seqkit + " grep -f " + outpath + "/tmp_work_dir/" + self.read_id + ".filt_ids > " + outpath + "/tmp_work_dir/" + self.read_id + "_inserts.fasta_filt"
			process = subprocess.Popen(command, shell=True)
			output, error = process.communicate()
			return(0)


	def consensus(self, seq, file_id, outpath, adapter_name, exclude_forward, cons_min_len, cons_max_len, iter_first_muscle, iter_second_muscle, stats, stats_per_read, lock): # overall main function to call the consensus
		with lock:		
			Stat.add_data_read_stat(stats_per_read, self.read_id, "dir", self.track_inserts)
		if(exclude_forward == True):
			if(self.track_inserts == "I_FORWARD"):
				with lock:
					Stat.inc_key("only_forward_inserts", stats)
					Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "only_forward_inserts")
					with open(outpath + "/" + file_id + "_removed_reads.fasta", 'a') as out_f:
						out_f.write(">" + self.read_id + " onlyForwardInserts" + "\n")
						out_f.write(seq + "\n")
				self.cleanup_read(outpath)
				return
		if(self.track_inserts == "I_PROBLEMATIC"):
			with lock:
				Stat.inc_key("problematic_insert_orientation", stats)
				Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "problematic_insert_orientation")
				with open(outpath + "/" + file_id + "_removed_reads.fasta", 'a') as out_f:
					out_f.write(">" + self.read_id + " problematicInsertOrientation" + "\n")
					out_f.write(seq + "\n")
			self.cleanup_read(outpath)
			return

		for num,insert in enumerate(self.inserts):
			with open(outpath + "/tmp_work_dir/" + self.read_id + "_inserts.fasta", 'a') as f:
				f.write(">insert_" + str(num) + insert.orientation + '\n')
				if(insert.orientation == "+"):
					f.write(seq[insert.start_query:insert.end_query] + '\n')
				else:
					f.write(str(Seq(seq[insert.start_query:insert.end_query], IUPAC.unambiguous_dna).reverse_complement()) + '\n')

		if(len(self.inserts) > 1): #  check if we have at least two inserts found
			self.muscle(outpath, False, iter_first_muscle) # first run of MUSCLE
			ret_val = self.filter_good_align(outpath) # kick out inserts with bad alignment/qual.
			if(ret_val == -1): # check if we have to skip this read because alignment is very bad
				with lock:
					Stat.inc_key("bad_MSA", stats)
					Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "bad_MSA")
					with open(outpath + "/" + file_id + "_removed_reads.fasta", 'a') as out_f:
						out_f.write(">" + self.read_id + " badMSA" + "\n")
						out_f.write(seq + "\n")
				self.cleanup_read(outpath)
				return

			self.muscle(outpath, True, iter_second_muscle) # second run of MUSCLE with filtered inserts
			self.cons(outpath) # call consensus with MUSCLE alignment from above

			# Check if the consensus is just the adapter (or contains adapter)
			result_check = ""			
			with open(outpath + "/tmp_work_dir/" + self.read_id + ".cons", 'r') as in_f:
				line = in_f.readline()
				while(line):
					if(line[0] != '>'):
						result_check = result_check + ''.join(c for c in line.rstrip() if c not in 'acgtnN-')											
					line = in_f.readline()
			alignment_check_adapter = (cpp_function_wrappers.adapter_alignment(result_check,
                                                   parameter.adapter[adapter_name]["Seq"],
                                                   parameter.adapter[adapter_name]["ScoringScheme"],
                                                   parameter.adapter[adapter_name]["Threshold"]-10)).split("\n")
			if(len(alignment_check_adapter[:-1]) > 0):
				with lock:
					Stat.inc_key("adapter_as_insert", stats)
					Stat.add_data_read_stat(stats_per_read, self.read_id, "len_cons", len(result_check))
					Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "adapter_as_insert")
					with open(outpath + "/" + file_id + "_removed_reads.fasta", 'a') as out_f:
						out_f.write(">" + self.read_id + " adapterAsInsert" + "\n")
						out_f.write(seq + "\n")
				self.cleanup_read(outpath)
				return
			# Check if the consensus length is within the desired range:
			if( (cons_min_len > len(result_check)) or (cons_max_len < len(result_check))):
				with lock:
					Stat.inc_key("consensus_size_out_of_range", stats)
					Stat.add_data_read_stat(stats_per_read, self.read_id, "len_cons", len(result_check))
					Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "consensus_out_of_range")
					with open(outpath + "/" + file_id + "_removed_reads.fasta", 'a') as out_f:
						out_f.write(">" + self.read_id + " ConsensusLengthOutOfRange" + "\n")
						out_f.write(seq + "\n")
				self.cleanup_read(outpath)
				return

			result_sequence = ""
			with lock: # write the resulting consensus to the output file. we have to "lock" this part when executing to make it thread-save.
				# count how many proper consensus sequences are generated:
				Stat.inc_key("cnt_proper_consensus", stats)
				with open(outpath + "/tmp_work_dir/" + self.read_id + ".cons", 'r') as in_f:
					with open(outpath + "/" + file_id + ".fasta", 'a') as out_f:
						line = in_f.readline()
						while(line):
							if(line[0] == '>'):
								out_f.write(">" + self.read_id + " " + self.read_desc + " " + self.track_adapters + " " + self.track_inserts + " " + adapter_name + '\n')
							else:
								result_sequence = result_sequence + ''.join(c for c in line.rstrip() if c not in 'acgtnN-')											
							line = in_f.readline()
						out_f.write(result_sequence + '\n')
				Stat.add_data_read_stat(stats_per_read, self.read_id, "len_cons", len(result_sequence))
				Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "consensus")
		elif(len(self.inserts) == 1): # if we have only one insert, we can not perform any MSA. Then execute this part. This part might not be run, because we MIGHT filter reads with less than 2 inserts found.
			with lock: # lock this part to make it thread save
				Stat.inc_key("cnt_proper_consensus", stats)
				with open(outpath + "/tmp_work_dir/" + self.read_id + "_inserts.fasta", 'r') as in_f:
					with open(outpath + "/" + file_id + ".fasta", 'a') as out_f:
						line = in_f.readline()
						while(line):
							if(line[0] == '>'):
								out_f.write(">" + self.read_id + " " + self.read_desc + " " + self.track_adapters + " " + self.track_inserts + '\n')
							else:
								out_f.write(line)
								Stat.add_data_read_stat(stats_per_read, self.read_id, "len_cons", len(line))
								Stat.add_data_read_stat(stats_per_read, self.read_id, "final_state", "OK")			
							line = in_f.readline()
		
		self.cleanup_read(outpath) # make a cleanup of the temp. files for this read. otherwise we accumulate tons of files if we are running a large analysis.


	def cleanup_all(rmpath):
		if(os.path.exists(rmpath + "/tmp_work_dir")):
			shutil.rmtree(rmpath + "/tmp_work_dir")


	def cleanup_read(self, rmpath):
		filelist = os.listdir(rmpath + "/tmp_work_dir")
		for one_file in filelist:
			if(one_file.startswith(self.read_id)):
				os.remove(os.path.join(rmpath + "/tmp_work_dir", one_file))

